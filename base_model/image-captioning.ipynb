{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Initial Imports and loading the utils function. The dataset is used is Flickr 30k</a> from kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T11:38:36.545116Z",
     "iopub.status.busy": "2024-12-12T11:38:36.544767Z",
     "iopub.status.idle": "2024-12-12T11:38:37.542727Z",
     "shell.execute_reply": "2024-12-12T11:38:37.542092Z",
     "shell.execute_reply.started": "2024-12-12T11:38:36.545070Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import pandas as pd\n",
    "import os\n",
    "from collections import Counter\n",
    "import spacy\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import torchvision.transforms as T\n",
    "import pickle\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T11:38:37.545012Z",
     "iopub.status.busy": "2024-12-12T11:38:37.544672Z",
     "iopub.status.idle": "2024-12-12T11:38:38.546573Z",
     "shell.execute_reply": "2024-12-12T11:38:38.545779Z",
     "shell.execute_reply.started": "2024-12-12T11:38:37.544975Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_data_location =  \"../input/flickr30k\"\n",
    "!ls $data_location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>2) Writing the custom dataset</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T11:38:38.548215Z",
     "iopub.status.busy": "2024-12-12T11:38:38.547941Z",
     "iopub.status.idle": "2024-12-12T11:38:39.140973Z",
     "shell.execute_reply": "2024-12-12T11:38:39.140205Z",
     "shell.execute_reply.started": "2024-12-12T11:38:38.548185Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "spacy_eng = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T11:38:39.142399Z",
     "iopub.status.busy": "2024-12-12T11:38:39.142133Z",
     "iopub.status.idle": "2024-12-12T11:38:39.153866Z",
     "shell.execute_reply": "2024-12-12T11:38:39.152851Z",
     "shell.execute_reply.started": "2024-12-12T11:38:39.142373Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# SOS: start of sentence, EOS: end of sentence, PAD: padding token, UNK: unknown\n",
    "class Vocabulary:\n",
    "    def __init__(self,freq_threshold):\n",
    "        self.itos = {0:\"<PAD>\",1:\"<SOS>\",2:\"<EOS>\",3:\"<UNK>\"}\n",
    "        self.stoi = {v:k for k,v in self.itos.items()}\n",
    "        self.freq_threshold = freq_threshold\n",
    "        \n",
    "    def __len__(self): \n",
    "        return len(self.itos)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tokenize(text):\n",
    "        return [token.text.lower() for token in spacy_eng.tokenizer(str(text))]\n",
    "    \n",
    "    def build_vocab(self, sentence_list):\n",
    "        frequencies = Counter()\n",
    "        idx = 4\n",
    "        \n",
    "        for sentence in sentence_list:\n",
    "            for word in self.tokenize(sentence):\n",
    "                frequencies[word] += 1\n",
    "                \n",
    "                if frequencies[word] == self.freq_threshold:\n",
    "                    self.stoi[word] = idx\n",
    "                    self.itos[idx] = word\n",
    "                    idx += 1\n",
    "    \n",
    "    def numericalize(self,text):\n",
    "        tokenized_text = self.tokenize(text)\n",
    "        return [ self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"] for token in tokenized_text ]  \n",
    "\n",
    "    def save_vocab(self, filepath):\n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump({'itos': self.itos, 'stoi': self.stoi}, f)\n",
    "\n",
    "    def load_vocab(self, filepath):\n",
    "        with open(filepath, 'rb') as f:\n",
    "            vocab_data = pickle.load(f)\n",
    "            self.itos = vocab_data['itos']\n",
    "            self.stoi = vocab_data['stoi']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T11:48:37.617556Z",
     "iopub.status.busy": "2024-12-12T11:48:37.617224Z",
     "iopub.status.idle": "2024-12-12T11:48:37.630148Z",
     "shell.execute_reply": "2024-12-12T11:48:37.629336Z",
     "shell.execute_reply.started": "2024-12-12T11:48:37.617529Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class FlickrDataset(Dataset):\n",
    "    \"\"\"\n",
    "    FlickrDataset\n",
    "    \"\"\"\n",
    "    def __init__(self,root_dir,captions_file,test,transform=None,freq_threshold=5):\n",
    "        self.root_dir = root_dir\n",
    "        self.df = pd.read_csv(captions_file)\n",
    "        self.transform = transform\n",
    "\n",
    "        split_ratio = 0.8\n",
    "        total_samples = len(self.df)\n",
    "\n",
    "        split_index = int(total_samples * split_ratio)\n",
    "        self.df = self.df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "        self.full_captions = self.df[\"caption\"]\n",
    "        \n",
    "        self.imgs = self.df[\"image\"][:split_index]\n",
    "        self.imgs_test = self.df[\"image\"][split_index:]\n",
    "        self.captions = self.df[\"caption\"][:split_index]\n",
    "        self.captions_test = self.df[\"caption\"][split_index:]\n",
    "        self.test = test\n",
    "        \n",
    "        self.vocab = Vocabulary(freq_threshold)\n",
    "        self.vocab.build_vocab(self.full_captions.tolist())\n",
    "        self.vocab.save_vocab('flickr30k_vocab.pkl')\n",
    "        \n",
    "    def __len__(self):\n",
    "        if self.test == True:\n",
    "            return len(self.imgs_test)\n",
    "        return len(self.imgs)\n",
    "    \n",
    "    def __getitem__(self,idx: int):\n",
    "        if self.test == True:\n",
    "            caption = self.captions_test.iloc[idx]\n",
    "            img_id = self.imgs_test.iloc[idx]\n",
    "        else:\n",
    "            caption = self.captions.iloc[idx]\n",
    "            img_id = self.imgs.iloc[idx]\n",
    "        \n",
    "        \n",
    "        img_location = os.path.join(self.root_dir,img_id)\n",
    "        img = Image.open(img_location).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        caption_vec = []\n",
    "        caption_vec += [self.vocab.stoi[\"<SOS>\"]]\n",
    "        caption_vec += self.vocab.numericalize(caption)\n",
    "        caption_vec += [self.vocab.stoi[\"<EOS>\"]]\n",
    "        \n",
    "        return img, torch.tensor(caption_vec)\n",
    "\n",
    "    def get_dataset_sizes(self):\n",
    "        train_size = len(self.imgs)\n",
    "        valid_size = len(self.imgs_test)\n",
    "        return train_size, valid_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T11:38:39.172270Z",
     "iopub.status.busy": "2024-12-12T11:38:39.171868Z",
     "iopub.status.idle": "2024-12-12T11:38:39.183393Z",
     "shell.execute_reply": "2024-12-12T11:38:39.182592Z",
     "shell.execute_reply.started": "2024-12-12T11:38:39.172211Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "transforms1 = T.Compose([\n",
    "    T.Resize((224,224)),\n",
    "    T.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T11:38:39.184986Z",
     "iopub.status.busy": "2024-12-12T11:38:39.184624Z",
     "iopub.status.idle": "2024-12-12T11:38:39.199872Z",
     "shell.execute_reply": "2024-12-12T11:38:39.199138Z",
     "shell.execute_reply.started": "2024-12-12T11:38:39.184951Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def show_image(inp, title=None):\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T11:38:39.201616Z",
     "iopub.status.busy": "2024-12-12T11:38:39.201223Z",
     "iopub.status.idle": "2024-12-12T11:38:39.212669Z",
     "shell.execute_reply": "2024-12-12T11:38:39.211986Z",
     "shell.execute_reply.started": "2024-12-12T11:38:39.201583Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CapsCollate:\n",
    "    def __init__(self,pad_idx,batch_first=False):\n",
    "        self.pad_idx = pad_idx\n",
    "        self.batch_first = batch_first\n",
    "    \n",
    "    def __call__(self,batch):\n",
    "        imgs = [item[0].unsqueeze(0) for item in batch]\n",
    "        imgs = torch.cat(imgs,dim=0)\n",
    "        \n",
    "        targets = [item[1] for item in batch]\n",
    "        targets = pad_sequence(targets, batch_first=self.batch_first, padding_value=self.pad_idx)\n",
    "        return imgs,targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T11:38:39.213852Z",
     "iopub.status.busy": "2024-12-12T11:38:39.213634Z",
     "iopub.status.idle": "2024-12-12T11:38:39.223986Z",
     "shell.execute_reply": "2024-12-12T11:38:39.223315Z",
     "shell.execute_reply.started": "2024-12-12T11:38:39.213831Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_data_loader(dataset,batch_size,shuffle=False,num_workers=1):\n",
    "    pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n",
    "    collate_fn = CapsCollate(pad_idx=pad_idx,batch_first=True)\n",
    "\n",
    "    data_loader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "trusted": true
   },
   "source": [
    "### 2) **<b>Implementing the Helper function to plot the Tensor image**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T11:38:39.225453Z",
     "iopub.status.busy": "2024-12-12T11:38:39.225176Z",
     "iopub.status.idle": "2024-12-12T11:38:39.236674Z",
     "shell.execute_reply": "2024-12-12T11:38:39.236028Z",
     "shell.execute_reply.started": "2024-12-12T11:38:39.225430Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#show the tensor image\n",
    "import matplotlib.pyplot as plt\n",
    "def show_image(img, title=None):\n",
    "    # unnormalize\n",
    "    img[0] = img[0] * 0.229\n",
    "    img[1] = img[1] * 0.224 \n",
    "    img[2] = img[2] * 0.225 \n",
    "    img[0] += 0.485 \n",
    "    img[1] += 0.456 \n",
    "    img[2] += 0.406\n",
    "    \n",
    "    img = img.numpy().transpose((1, 2, 0))\n",
    "    \n",
    "    \n",
    "    plt.imshow(img)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T11:49:12.876207Z",
     "iopub.status.busy": "2024-12-12T11:49:12.875751Z",
     "iopub.status.idle": "2024-12-12T11:50:36.519276Z",
     "shell.execute_reply": "2024-12-12T11:50:36.518362Z",
     "shell.execute_reply.started": "2024-12-12T11:49:12.876149Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Initiate the Dataset and Dataloader\n",
    "BATCH_SIZE = 256\n",
    "NUM_WORKER = 4\n",
    "\n",
    "#defining the transform to be applied\n",
    "transforms = T.Compose([\n",
    "    T.Resize(226),                     \n",
    "    T.RandomCrop(224),                 \n",
    "    T.ToTensor(),                               \n",
    "    T.Normalize((0.485, 0.456, 0.406),(0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "\n",
    "train_dataset =  FlickrDataset(\n",
    "    root_dir = train_data_location+\"/Images\",\n",
    "    captions_file = train_data_location+\"/captions.txt\",\n",
    "    transform=transforms,\n",
    "    test = False\n",
    ")\n",
    "\n",
    "train_data_loader = get_data_loader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKER,\n",
    "    shuffle=True,\n",
    ")\n",
    "valid_dataset =  FlickrDataset(\n",
    "    root_dir = train_data_location+\"/Images\",\n",
    "    captions_file = train_data_location+\"/captions.txt\",\n",
    "    transform=transforms, \n",
    "    test = True\n",
    ")\n",
    "\n",
    "valid_data_loader = get_data_loader(\n",
    "    dataset=valid_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKER,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T11:51:31.519008Z",
     "iopub.status.busy": "2024-12-12T11:51:31.518708Z",
     "iopub.status.idle": "2024-12-12T11:51:31.525775Z",
     "shell.execute_reply": "2024-12-12T11:51:31.524953Z",
     "shell.execute_reply.started": "2024-12-12T11:51:31.518983Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "vocab_size = len(valid_dataset.vocab)\n",
    "print(vocab_size)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T11:53:57.225791Z",
     "iopub.status.busy": "2024-12-12T11:53:57.225491Z",
     "iopub.status.idle": "2024-12-12T11:53:57.230162Z",
     "shell.execute_reply": "2024-12-12T11:53:57.229321Z",
     "shell.execute_reply.started": "2024-12-12T11:53:57.225766Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(train_dataset.get_dataset_sizes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T11:32:07.935689Z",
     "iopub.status.busy": "2024-12-12T11:32:07.935376Z",
     "iopub.status.idle": "2024-12-12T11:32:07.939978Z",
     "shell.execute_reply": "2024-12-12T11:32:07.939117Z",
     "shell.execute_reply.started": "2024-12-12T11:32:07.935664Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(len(valid_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "trusted": true
   },
   "source": [
    "### 3) Defining the Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T08:26:13.917309Z",
     "iopub.status.busy": "2024-12-12T08:26:13.916985Z",
     "iopub.status.idle": "2024-12-12T08:26:13.924677Z",
     "shell.execute_reply": "2024-12-12T08:26:13.923708Z",
     "shell.execute_reply.started": "2024-12-12T08:26:13.917282Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        resnet = models.resnet101(pretrained=True)\n",
    "        for param in resnet.parameters():\n",
    "            param.requires_grad_(False)\n",
    "        \n",
    "        modules = list(resnet.children())[:-2]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        \n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.resnet(images)                                    #(batch_size,2048,7,7)\n",
    "        features = features.permute(0, 2, 3, 1)                           #(batch_size,7,7,2048)\n",
    "        features = features.view(features.size(0), -1, features.size(-1)) #(batch_size,49,2048)\n",
    "        return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T08:26:16.029054Z",
     "iopub.status.busy": "2024-12-12T08:26:16.028717Z",
     "iopub.status.idle": "2024-12-12T08:26:16.036374Z",
     "shell.execute_reply": "2024-12-12T08:26:16.035590Z",
     "shell.execute_reply.started": "2024-12-12T08:26:16.029025Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Bahdanau Attention\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, encoder_dim,decoder_dim,attention_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        self.attention_dim = attention_dim\n",
    "        \n",
    "        self.W = nn.Linear(decoder_dim,attention_dim)\n",
    "        self.U = nn.Linear(encoder_dim,attention_dim)\n",
    "        \n",
    "        self.A = nn.Linear(attention_dim,1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, features, hidden_state):\n",
    "        u_hs = self.U(features)     #(batch_size,num_layers,attention_dim)\n",
    "        w_ah = self.W(hidden_state) #(batch_size,attention_dim)\n",
    "        \n",
    "        combined_states = torch.tanh(u_hs + w_ah.unsqueeze(1)) #(batch_size,num_layers,attemtion_dim)\n",
    "        \n",
    "        attention_scores = self.A(combined_states)         #(batch_size,num_layers,1)\n",
    "        attention_scores = attention_scores.squeeze(2)     #(batch_size,num_layers)\n",
    "        \n",
    "        \n",
    "        alpha = F.softmax(attention_scores,dim=1)          #(batch_size,num_layers)\n",
    "        \n",
    "        attention_weights = features * alpha.unsqueeze(2)  #(batch_size,num_layers,features_dim)\n",
    "        attention_weights = attention_weights.sum(dim=1)   #(batch_size,num_layers)\n",
    "        \n",
    "        return alpha,attention_weights\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T08:26:18.364221Z",
     "iopub.status.busy": "2024-12-12T08:26:18.363921Z",
     "iopub.status.idle": "2024-12-12T08:26:18.382055Z",
     "shell.execute_reply": "2024-12-12T08:26:18.381334Z",
     "shell.execute_reply.started": "2024-12-12T08:26:18.364194Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self,embed_size, vocab_size, attention_dim,encoder_dim,decoder_dim,drop_prob=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.attention_dim = attention_dim\n",
    "        self.decoder_dim = decoder_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size,embed_size)\n",
    "        self.attention = Attention(encoder_dim,decoder_dim,attention_dim)\n",
    "        \n",
    "        \n",
    "        self.init_h = nn.Linear(encoder_dim, decoder_dim)  \n",
    "        self.init_c = nn.Linear(encoder_dim, decoder_dim)  \n",
    "        self.lstm_cell = nn.LSTMCell(embed_size+encoder_dim,decoder_dim,bias=True)\n",
    "        self.f_beta = nn.Linear(decoder_dim, encoder_dim)\n",
    "        \n",
    "        \n",
    "        self.fcn = nn.Linear(decoder_dim,vocab_size)\n",
    "        self.drop = nn.Dropout(drop_prob)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def forward(self, features, captions):\n",
    "        \n",
    "        embeds = self.embedding(captions)\n",
    "        \n",
    "        h, c = self.init_hidden_state(features)  \n",
    "        \n",
    "        seq_length = len(captions[0])-1 \n",
    "        batch_size = captions.size(0)\n",
    "        num_features = features.size(1)\n",
    "        \n",
    "        preds = torch.zeros(batch_size, seq_length, self.vocab_size).to(device)\n",
    "        alphas = torch.zeros(batch_size, seq_length,num_features).to(device)\n",
    "                \n",
    "        for s in range(seq_length):\n",
    "            alpha,context = self.attention(features, h)\n",
    "            lstm_input = torch.cat((embeds[:, s], context), dim=1)\n",
    "            h, c = self.lstm_cell(lstm_input, (h, c))\n",
    "                    \n",
    "            output = self.fcn(self.drop(h))\n",
    "            \n",
    "            preds[:,s] = output\n",
    "            alphas[:,s] = alpha  \n",
    "        \n",
    "        \n",
    "        return preds, alphas\n",
    "    \n",
    "    def generate_caption(self,features,max_len=20,vocab=None):\n",
    "        \n",
    "        batch_size = features.size(0)\n",
    "        h, c = self.init_hidden_state(features)  \n",
    "        \n",
    "        alphas = []\n",
    "        \n",
    "        word = torch.tensor(vocab.stoi['<SOS>']).view(1,-1).to(device)\n",
    "        embeds = self.embedding(word)\n",
    "\n",
    "        captions = []\n",
    "        \n",
    "        for i in range(max_len):\n",
    "            alpha,context = self.attention(features, h)\n",
    "            \n",
    "            alphas.append(alpha.cpu().detach().numpy())\n",
    "            \n",
    "            lstm_input = torch.cat((embeds[:, 0], context), dim=1)\n",
    "            h, c = self.lstm_cell(lstm_input, (h, c))\n",
    "            output = self.fcn(self.drop(h))\n",
    "            output = output.view(batch_size,-1)\n",
    "        \n",
    "            predicted_word_idx = output.argmax(dim=1)\n",
    "            \n",
    "            captions.append(predicted_word_idx.item())\n",
    "            \n",
    "            if vocab.itos[predicted_word_idx.item()] == \"<EOS>\":\n",
    "                break\n",
    "            \n",
    "            embeds = self.embedding(predicted_word_idx.unsqueeze(0))\n",
    "        \n",
    "        return [vocab.itos[idx] for idx in captions],alphas\n",
    "    \n",
    "    \n",
    "    def init_hidden_state(self, encoder_out):\n",
    "        mean_encoder_out = encoder_out.mean(dim=1)\n",
    "        h = self.init_h(mean_encoder_out)  \n",
    "        c = self.init_c(mean_encoder_out)\n",
    "        return h, c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T19:43:12.606713Z",
     "iopub.status.busy": "2024-12-11T19:43:12.606435Z",
     "iopub.status.idle": "2024-12-11T19:43:12.612507Z",
     "shell.execute_reply": "2024-12-11T19:43:12.611493Z",
     "shell.execute_reply.started": "2024-12-11T19:43:12.606690Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self,embed_size, vocab_size, attention_dim,encoder_dim,decoder_dim,drop_prob=0.3):\n",
    "        super().__init__()\n",
    "        self.encoder = EncoderCNN()\n",
    "        self.decoder = DecoderRNN(\n",
    "            embed_size=embed_size,\n",
    "            vocab_size = vocab_size,\n",
    "            attention_dim=attention_dim,\n",
    "            encoder_dim=encoder_dim,\n",
    "            decoder_dim=decoder_dim\n",
    "        )\n",
    "        \n",
    "    def forward(self, images, captions):\n",
    "        features = self.encoder(images)\n",
    "        outputs = self.decoder(features, captions)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Setting Hypperparameter and Init the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T19:45:03.077003Z",
     "iopub.status.busy": "2024-12-11T19:45:03.076734Z",
     "iopub.status.idle": "2024-12-11T19:45:03.080884Z",
     "shell.execute_reply": "2024-12-11T19:45:03.079995Z",
     "shell.execute_reply.started": "2024-12-11T19:45:03.076980Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Hyperparams\n",
    "embed_size=300\n",
    "vocab_size = len(train_dataset.vocab)\n",
    "attention_dim=256\n",
    "encoder_dim=2048\n",
    "decoder_dim=512\n",
    "learning_rate = 3e-4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T19:45:04.665837Z",
     "iopub.status.busy": "2024-12-11T19:45:04.665523Z",
     "iopub.status.idle": "2024-12-11T19:45:05.955928Z",
     "shell.execute_reply": "2024-12-11T19:45:05.954971Z",
     "shell.execute_reply.started": "2024-12-11T19:45:04.665810Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#init model\n",
    "model = EncoderDecoder(\n",
    "    embed_size=embed_size,\n",
    "    vocab_size = vocab_size,\n",
    "    attention_dim=attention_dim,\n",
    "    encoder_dim=encoder_dim,\n",
    "    decoder_dim=decoder_dim\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=train_dataset.vocab.stoi[\"<PAD>\"])\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T19:45:08.253473Z",
     "iopub.status.busy": "2024-12-11T19:45:08.253172Z",
     "iopub.status.idle": "2024-12-11T19:45:08.257828Z",
     "shell.execute_reply": "2024-12-11T19:45:08.256956Z",
     "shell.execute_reply.started": "2024-12-11T19:45:08.253447Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#helper function to save the model\n",
    "def save_model(model,num_epochs):\n",
    "    model_state = {\n",
    "        'num_epochs':num_epochs,\n",
    "        'embed_size':embed_size,\n",
    "        'vocab_size':vocab_size,\n",
    "        'attention_dim':attention_dim,\n",
    "        'encoder_dim':encoder_dim,\n",
    "        'decoder_dim':decoder_dim,\n",
    "        'state_dict':model.state_dict()\n",
    "    }\n",
    "\n",
    "    torch.save(model_state,'attention_model_state.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Training Job from above configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T21:13:15.174605Z",
     "iopub.status.busy": "2024-12-11T21:13:15.174320Z",
     "iopub.status.idle": "2024-12-11T21:13:17.694097Z",
     "shell.execute_reply": "2024-12-11T21:13:17.693138Z",
     "shell.execute_reply.started": "2024-12-11T21:13:15.174575Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "!wandb login ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T21:15:29.201201Z",
     "iopub.status.busy": "2024-12-11T21:15:29.200858Z",
     "iopub.status.idle": "2024-12-11T21:15:29.444690Z",
     "shell.execute_reply": "2024-12-11T21:15:29.443997Z",
     "shell.execute_reply.started": "2024-12-11T21:15:29.201170Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "wandb.init(project = 'image-captioning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T20:17:56.056224Z",
     "iopub.status.busy": "2024-12-11T20:17:56.055886Z",
     "iopub.status.idle": "2024-12-11T20:33:44.769740Z",
     "shell.execute_reply": "2024-12-11T20:33:44.768813Z",
     "shell.execute_reply.started": "2024-12-11T20:17:56.056191Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 40\n",
    "print_every = 100\n",
    "patience = 3  # Number of epochs to wait for improvement in validation loss\n",
    "best_valid_loss = float(\"inf\")  # Initialize best validation loss\n",
    "epochs_without_improvement = 0 \n",
    "\n",
    "for epoch in range(1,num_epochs+1):  \n",
    "    model.train()\n",
    "    epoch_train_loss = 0\n",
    "    for idx, (image, captions) in enumerate(iter(train_data_loader)):\n",
    "        image,captions = image.to(device),captions.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs,attentions = model(image, captions)\n",
    "        targets = captions[:,1:]\n",
    "        loss = criterion(outputs.view(-1, vocab_size), targets.reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_train_loss += loss.item()\n",
    "\n",
    "        if (idx+1)%print_every == 0:\n",
    "            print(\"Train section\")\n",
    "            print(\"Epoch: {} |Loss: {:.5f}\".format(epoch,loss.item()))\n",
    "            \n",
    "    wandb.log({\"train_loss\": epoch_train_loss / len(train_data_loader), \"epoch\": epoch})\n",
    "    model.eval()  \n",
    "\n",
    "    epoch_valid_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for idx, (image, captions) in enumerate(iter(valid_data_loader)):\n",
    "            image,captions = image.to(device),captions.to(device)\n",
    "            outputs,attentions = model(image, captions)\n",
    "            targets = captions[:,1:]\n",
    "            loss = criterion(outputs.view(-1, vocab_size), targets.reshape(-1))\n",
    "\n",
    "            epoch_valid_loss += loss.item()\n",
    "    \n",
    "            if (idx+1)%print_every == 0:\n",
    "                print(\"Valid section\")\n",
    "                print(\"Epoch: {} |Loss: {:.5f}\".format(epoch,loss.item()))\n",
    "                \n",
    "    wandb.log({\"valid_loss\": epoch_valid_loss / len(valid_data_loader), \"epoch\": epoch})\n",
    "    \n",
    "    if epoch_valid_loss < best_valid_loss:\n",
    "        best_valid_loss = epoch_valid_loss\n",
    "        epochs_without_improvement = 0  # Reset the counter if there's improvement\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "    \n",
    "    if epochs_without_improvement >= patience:\n",
    "        print(f\"Early stopping at epoch {epoch} due to no improvement in validation loss.\")\n",
    "        break\n",
    "    save_model(model,epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Visualizing the attentions\n",
    "Defining helper functions\n",
    "<li>Given the image generate captions and attention scores</li>\n",
    "<li>Plot the attention scores in the image</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T20:39:30.737536Z",
     "iopub.status.busy": "2024-12-11T20:39:30.736955Z",
     "iopub.status.idle": "2024-12-11T20:39:30.759638Z",
     "shell.execute_reply": "2024-12-11T20:39:30.758658Z",
     "shell.execute_reply.started": "2024-12-11T20:39:30.737480Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#generate caption\n",
    "def get_caps_from(features_tensors):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        features = model.encoder(features_tensors.to(device))\n",
    "        caps,alphas = model.decoder.generate_caption(features,vocab=train_dataset.vocab)\n",
    "        caption = ' '.join(caps)\n",
    "        show_image(features_tensors[0],title=caption)\n",
    "    \n",
    "    return caps,alphas\n",
    "\n",
    "#Show attention\n",
    "def plot_attention(img, result, attention_plot):\n",
    "    img[0] = img[0] * 0.229\n",
    "    img[1] = img[1] * 0.224 \n",
    "    img[2] = img[2] * 0.225 \n",
    "    img[0] += 0.485 \n",
    "    img[1] += 0.456 \n",
    "    img[2] += 0.406\n",
    "    \n",
    "    img = img.numpy().transpose((1, 2, 0))\n",
    "    temp_image = img\n",
    "\n",
    "    fig = plt.figure(figsize=(15, 15))\n",
    "\n",
    "    len_result = len(result)\n",
    "    for l in range(len_result):\n",
    "        temp_att = attention_plot[l].reshape(7,7)\n",
    "        \n",
    "        ax = fig.add_subplot(len_result//2,len_result//2, l+1)\n",
    "        ax.set_title(result[l])\n",
    "        img = ax.imshow(temp_image)\n",
    "        ax.imshow(temp_att, cmap='gray', alpha=0.7, extent=img.get_extent())\n",
    "        \n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T20:39:52.834294Z",
     "iopub.status.busy": "2024-12-11T20:39:52.833834Z",
     "iopub.status.idle": "2024-12-11T20:40:00.823612Z",
     "shell.execute_reply": "2024-12-11T20:40:00.819758Z",
     "shell.execute_reply.started": "2024-12-11T20:39:52.834238Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Example of generated caption\n",
    "dataiter = iter(train_data_loader)\n",
    "images,_ = next(dataiter)\n",
    "\n",
    "img = images[0].detach().clone()\n",
    "img1 = images[0].detach().clone()\n",
    "caps,alphas = get_caps_from(img.unsqueeze(0))\n",
    "\n",
    "plot_attention(img1, caps, alphas)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 623289,
     "sourceId": 1111676,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 623329,
     "sourceId": 1111749,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 868251,
     "sourceId": 1479515,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30009,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
