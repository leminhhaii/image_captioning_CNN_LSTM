{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVUdv-UNYrBK"
      },
      "source": [
        "### 1) Initial Imports and loading the utils function. The dataset is used is <a href='https://www.kaggle.com/adityajn105/flickr8k'>Flickr 30k</a> from kaggle.<br>Custom dataset and dataloader is implemented in <a href=\"https://www.kaggle.com/mdteach/torch-data-loader-flicker-8k\">this</a> notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-10T22:19:07.796868Z",
          "iopub.status.busy": "2024-12-10T22:19:07.796058Z",
          "iopub.status.idle": "2024-12-10T22:19:10.247969Z",
          "shell.execute_reply": "2024-12-10T22:19:10.246982Z",
          "shell.execute_reply.started": "2024-12-10T22:19:07.796828Z"
        },
        "id": "q3aPRDtYYrBM",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#location of the data\n",
        "data_location =  \"../input/flickr30k\"\n",
        "!ls $data_location\n",
        "\n",
        "#reading the text data\n",
        "import pandas as pd\n",
        "caption_file = data_location + '/captions.txt'\n",
        "df = pd.read_csv(caption_file)\n",
        "print(\"There are {} image to captions\".format(len(df)))\n",
        "df.head(7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-10T22:19:10.250395Z",
          "iopub.status.busy": "2024-12-10T22:19:10.250009Z",
          "iopub.status.idle": "2024-12-10T22:19:10.594083Z",
          "shell.execute_reply": "2024-12-10T22:19:10.593278Z",
          "shell.execute_reply.started": "2024-12-10T22:19:10.250354Z"
        },
        "id": "_z9dTNVkYrBO",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "#select any index from the whole dataset\n",
        "#single image has 5 captions\n",
        "#so, select indx as: 1,6,11,16...\n",
        "data_idx = 11\n",
        "\n",
        "#eg path to be plot: ../input/flickr8k/Images/1000268201_693b08cb0e.jpg\n",
        "image_path = data_location+\"/Images/\"+df.iloc[data_idx,0]\n",
        "img=mpimg.imread(image_path)\n",
        "plt.imshow(img)\n",
        "plt.show()\n",
        "\n",
        "#image consits of 5 captions,\n",
        "#showing all 5 captions of the image of the given idx\n",
        "for i in range(data_idx,data_idx+5):\n",
        "    print(\"Caption:\",df.iloc[i,1])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGC7Am-2YrBO"
      },
      "source": [
        "<h2>2) Writing the custom dataset</h2>\n",
        "<p>Writing the custom torch dataset class so, that we can abastract out the dataloading steps during the training and validation process</p>\n",
        "<p>Here, dataloader is created which gives the batch of image and its captions with following processing done:</p>\n",
        "\n",
        "<li>caption word tokenized to unique numbers</li>\n",
        "<li>vocab instance created to store all the relivent words in the datasets</li>\n",
        "<li>each batch, caption padded to have same sequence length</li>\n",
        "<li>image resized to the desired size and converted into captions</li>\n",
        "\n",
        "<br><p>In this way the dataprocessing is done, and the dataloader is ready to be used with <b>Pytorch</b></p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-10T22:19:10.595399Z",
          "iopub.status.busy": "2024-12-10T22:19:10.595119Z",
          "iopub.status.idle": "2024-12-10T22:19:30.843606Z",
          "shell.execute_reply": "2024-12-10T22:19:30.842663Z",
          "shell.execute_reply.started": "2024-12-10T22:19:10.595364Z"
        },
        "id": "GlaOSxwSYrBO",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "!python -m spacy download en_core_web_sm\n",
        "import os\n",
        "from collections import Counter\n",
        "import spacy\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "import torchvision.transforms as T\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "#using spacy for the better text tokenization\n",
        "spacy_eng = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "#example\n",
        "text = \"Let's learn deep learning\"\n",
        "tokens = [token.text.lower() for token in spacy_eng.tokenizer(text)]\n",
        "print(tokens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qzj8InaFYrBO"
      },
      "source": [
        "***xây dựng vocabulary cho bài toán***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-10T22:19:30.84602Z",
          "iopub.status.busy": "2024-12-10T22:19:30.845604Z",
          "iopub.status.idle": "2024-12-10T22:19:30.85354Z",
          "shell.execute_reply": "2024-12-10T22:19:30.852689Z",
          "shell.execute_reply.started": "2024-12-10T22:19:30.845993Z"
        },
        "id": "OnLKk5TYYrBP",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class Vocabulary:\n",
        "    def __init__(self,freq_threshold):\n",
        "        #setting the pre-reserved tokens int to string tokens\n",
        "        self.itos = {0:\"<PAD>\",1:\"<SOS>\",2:\"<EOS>\",3:\"<UNK>\"}\n",
        "\n",
        "        #string to int tokens\n",
        "        #its reverse dict self.itos\n",
        "        self.stoi = {v:k for k,v in self.itos.items()}\n",
        "\n",
        "        self.freq_threshold = freq_threshold # tần suất xuất hiện tối thiểu để thêm 1 từ vào từ điển\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.itos)\n",
        "\n",
        "    @staticmethod\n",
        "    def tokenize(text):\n",
        "        return [token.text.lower() for token in spacy_eng.tokenizer(str(text))]\n",
        "\n",
        "    def build_vocab(self, sentence_list):\n",
        "        frequencies = Counter() # hàm counter giúp đếm số lượng từ xuất hiện\n",
        "        idx = 4 # số tiếp theo của từ điển do đã có sẵn 0123\n",
        "\n",
        "        for sentence in sentence_list:\n",
        "            for word in self.tokenize(sentence):\n",
        "                frequencies[word] += 1\n",
        "\n",
        "                #add the word to the vocab if it reaches minum frequecy threshold\n",
        "                if frequencies[word] == self.freq_threshold:\n",
        "                    self.stoi[word] = idx\n",
        "                    self.itos[idx] = word\n",
        "                    idx += 1\n",
        "\n",
        "    def numericalize(self,text):\n",
        "        \"\"\" For each word in the text corresponding index token for that word form the vocab built as list \"\"\"\n",
        "        tokenized_text = self.tokenize(text)\n",
        "        return [ self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"] for token in tokenized_text ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-10T22:19:30.855026Z",
          "iopub.status.busy": "2024-12-10T22:19:30.854698Z",
          "iopub.status.idle": "2024-12-10T22:19:30.872809Z",
          "shell.execute_reply": "2024-12-10T22:19:30.87212Z",
          "shell.execute_reply.started": "2024-12-10T22:19:30.854991Z"
        },
        "id": "bpmfUaSTYrBP",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#testing the vicab class\n",
        "v = Vocabulary(freq_threshold=1)\n",
        "\n",
        "v.build_vocab([\"This is a good place to find a city\"])\n",
        "print(v.stoi)\n",
        "print(v.numericalize(\"This is a good place to find a city here!!\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfSk3Wv7YrBP"
      },
      "source": [
        "****Thiết lập dataset để truy cập lấy ảnh và caption đồng thời chuyển caption về dạng numerical tensor thông qua vocab đã xây dựng bên trên****"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-10T22:19:30.87406Z",
          "iopub.status.busy": "2024-12-10T22:19:30.87374Z",
          "iopub.status.idle": "2024-12-10T22:19:30.883931Z",
          "shell.execute_reply": "2024-12-10T22:19:30.883058Z",
          "shell.execute_reply.started": "2024-12-10T22:19:30.874027Z"
        },
        "id": "KiYaZRbsYrBQ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class FlickrDataset(Dataset):\n",
        "\n",
        "    def __init__(self,root_dir,captions_file,transform=None,freq_threshold=5):\n",
        "        self.root_dir = root_dir\n",
        "        self.df = pd.read_csv(caption_file)\n",
        "        self.transform = transform\n",
        "\n",
        "        #Get image and caption colum from the dataframe\n",
        "        self.imgs = self.df[\"image\"]\n",
        "        self.captions = self.df[\"caption\"]\n",
        "\n",
        "        #Initialize vocabulary and build vocab\n",
        "        self.vocab = Vocabulary(freq_threshold)\n",
        "        self.vocab.build_vocab(self.captions.tolist()) # build vocab từ caption\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "        caption = self.captions[idx] # lấy caption dựa trên index\n",
        "        img_name = self.imgs[idx] # lấy tên ảnh dựa trên index\n",
        "\n",
        "        # tạo đường dẫn đến tệp ảnh dựa vào tên ảnh đã có và đổi về dạng RGB\n",
        "        img_location = os.path.join(self.root_dir,img_name)\n",
        "        img = Image.open(img_location).convert(\"RGB\")\n",
        "\n",
        "        #apply the transfromation to the image\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        #numericalize the caption text\n",
        "        caption_vec = []\n",
        "        caption_vec += [self.vocab.stoi[\"<SOS>\"]]\n",
        "        caption_vec += self.vocab.numericalize(caption)\n",
        "        caption_vec += [self.vocab.stoi[\"<EOS>\"]]\n",
        "\n",
        "        return img, torch.tensor(caption_vec) # img là ảnh đã biến đổi về RGB\n",
        "                                              # caption_vec là tensor numerical của caption thông qua vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-10T22:19:30.885979Z",
          "iopub.status.busy": "2024-12-10T22:19:30.885155Z",
          "iopub.status.idle": "2024-12-10T22:19:30.898547Z",
          "shell.execute_reply": "2024-12-10T22:19:30.897755Z",
          "shell.execute_reply.started": "2024-12-10T22:19:30.885952Z"
        },
        "id": "I1DTtOvQYrBQ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#defing the transform to be applied\n",
        "transforms = T.Compose([\n",
        "    T.Resize((224,224)),\n",
        "    T.ToTensor() # chuyển về tensor và chuẩn hóa [0,1]\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-10T22:19:30.899806Z",
          "iopub.status.busy": "2024-12-10T22:19:30.899565Z",
          "iopub.status.idle": "2024-12-10T22:19:30.910196Z",
          "shell.execute_reply": "2024-12-10T22:19:30.909402Z",
          "shell.execute_reply.started": "2024-12-10T22:19:30.899783Z"
        },
        "id": "4IePfHr0YrBQ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def show_image(inp, title=None):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "    # chuyển tensor về numpy và đinh nghĩa lại thứ tự các chiều để plot\n",
        "    inp = inp.numpy().transpose((1, 2, 0)) # đổi c,h,w thành h,w,c (1,2,0)\n",
        "    plt.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-10T22:19:30.911909Z",
          "iopub.status.busy": "2024-12-10T22:19:30.911198Z",
          "iopub.status.idle": "2024-12-10T22:19:53.101883Z",
          "shell.execute_reply": "2024-12-10T22:19:53.101Z",
          "shell.execute_reply.started": "2024-12-10T22:19:30.911874Z"
        },
        "id": "AqzaIQ_lYrBQ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#testing the dataset class\n",
        "dataset =  FlickrDataset(\n",
        "    root_dir = data_location+\"/Images\",\n",
        "    captions_file = data_location+\"/captions.txt\",\n",
        "    transform=transforms\n",
        ")\n",
        "if '.' in dataset.vocab.stoi:\n",
        "    # Lấy chỉ số của token '.' trong từ điển stoi\n",
        "    idx = dataset.vocab.stoi['.']\n",
        "\n",
        "    # Xóa token '.' khỏi stoi và itos\n",
        "    del dataset.vocab.stoi['.']\n",
        "    del dataset.vocab.itos[idx]\n",
        "\n",
        "for i in range(10):\n",
        "    print(dataset.captions[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-10T22:19:53.104653Z",
          "iopub.status.busy": "2024-12-10T22:19:53.104369Z",
          "iopub.status.idle": "2024-12-10T22:19:53.552943Z",
          "shell.execute_reply": "2024-12-10T22:19:53.552139Z",
          "shell.execute_reply.started": "2024-12-10T22:19:53.10463Z"
        },
        "id": "Nop8i7bvYrBQ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "img, caps = dataset[5]\n",
        "show_image(img,\"Image\")\n",
        "print(\"Token:\",caps)\n",
        "print(\"Sentence:\")\n",
        "print([dataset.vocab.itos[token] for token in caps.tolist()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvVo1BroYrBQ"
      },
      "source": [
        "***Padding và chuyển về tensor 4 chiều***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-10T22:19:53.55414Z",
          "iopub.status.busy": "2024-12-10T22:19:53.553893Z",
          "iopub.status.idle": "2024-12-10T22:19:53.559759Z",
          "shell.execute_reply": "2024-12-10T22:19:53.558927Z",
          "shell.execute_reply.started": "2024-12-10T22:19:53.554116Z"
        },
        "id": "ADfraB3BYrBR",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# hàm giúp padding các caption và chuyển về tensor 4 chiếu\n",
        "class CapsCollate:\n",
        "    \"\"\"\n",
        "    Collate to apply the padding to the captions with dataloader\n",
        "    \"\"\"\n",
        "    def __init__(self,pad_idx,batch_first=False):\n",
        "        self.pad_idx = pad_idx # pad_idx là giá trị số dùng để padding\n",
        "        self.batch_first = batch_first\n",
        "        # nếu batch_first = True thì kích cỡ ma trận numerical của 1 caption có kích cỡ batch, lenght.\n",
        "        # False thì ngược lại\n",
        "\n",
        "    def __call__(self,batch): # batch là tập tuple(image,caption) của một batch\n",
        "        imgs = [item[0].unsqueeze(0) for item in batch] # thêm 1 chiều cho ảnh (C, H, W) sẽ trở thành (1, C, H, W)\n",
        "        imgs = torch.cat(imgs,dim=0) # ghép tất cả ảnh thành 1 tensor duy nhất (B, C, H, W) với B là số ảnh trong batch\n",
        "\n",
        "        targets = [item[1] for item in batch] # 1 list các tensor caption 1 chiều\n",
        "        targets = pad_sequence(targets, batch_first=self.batch_first, padding_value=self.pad_idx) # padding\n",
        "        return imgs,targets # trả về tensor 4d của ảnh và 2d của caption (batch, max_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-10T22:19:53.561016Z",
          "iopub.status.busy": "2024-12-10T22:19:53.560753Z",
          "iopub.status.idle": "2024-12-10T22:19:53.57678Z",
          "shell.execute_reply": "2024-12-10T22:19:53.575923Z",
          "shell.execute_reply.started": "2024-12-10T22:19:53.560973Z"
        },
        "id": "hBnhrXc2YrBR",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "# load data\n",
        "def get_data_loader(dataset,batch_size=4,shuffle=False,num_workers=1):\n",
        "    \"\"\"\n",
        "    Returns torch dataloader for the flicker8k dataset\n",
        "\n",
        "    Parameters\n",
        "    -----------\n",
        "    dataset: FlickrDataset (which was created above)\n",
        "        custom torchdataset named FlickrDataset\n",
        "    batch_size: int\n",
        "        number of data to load in a particular batch\n",
        "    shuffle: boolean,optional;\n",
        "        should shuffle the datasests (default is False)\n",
        "    num_workers: int,optional\n",
        "        numbers of workers to run (default is 1)\n",
        "    \"\"\"\n",
        "\n",
        "    pad_idx = dataset.vocab.stoi[\"<PAD>\"] # giá trị số của token padding\n",
        "    collate_fn = CapsCollate(pad_idx=pad_idx,batch_first=True) # padding\n",
        "\n",
        "    data_loader = DataLoader(\n",
        "        dataset=dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        num_workers=num_workers,\n",
        "        collate_fn=collate_fn\n",
        "    )\n",
        "\n",
        "    return data_loader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f48ec521-cf45-4fcc-b266-16de6ca56488",
        "_uuid": "b60eed4a-4184-49f4-b1fb-af2c50754b1e",
        "execution": {
          "iopub.execute_input": "2024-12-10T22:19:53.577898Z",
          "iopub.status.busy": "2024-12-10T22:19:53.57769Z",
          "iopub.status.idle": "2024-12-10T22:19:53.59033Z",
          "shell.execute_reply": "2024-12-10T22:19:53.589585Z",
          "shell.execute_reply.started": "2024-12-10T22:19:53.577876Z"
        },
        "id": "qDpIKowLYrBR",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#location of the training data\n",
        "data_location =  \"../input/flickr30k\"\n",
        "\n",
        "#imports\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "import torchvision.transforms as T\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqMEBLVjYrBR",
        "trusted": true
      },
      "source": [
        "### 2) **<b>Implementing the Helper function to plot the Tensor image**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-10T22:19:53.591392Z",
          "iopub.status.busy": "2024-12-10T22:19:53.591182Z",
          "iopub.status.idle": "2024-12-10T22:19:53.60026Z",
          "shell.execute_reply": "2024-12-10T22:19:53.599491Z",
          "shell.execute_reply.started": "2024-12-10T22:19:53.59137Z"
        },
        "id": "W4EU_GS1YrBR",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#show the tensor image bằng cách chuyển tensor về dạng trước khi chuẩn hóa rồi plot\n",
        "import matplotlib.pyplot as plt\n",
        "def show_image(img, title=None):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "\n",
        "    #unnormalize\n",
        "    img[0] = img[0] * 0.229\n",
        "    img[1] = img[1] * 0.224\n",
        "    img[2] = img[2] * 0.225\n",
        "    img[0] += 0.485\n",
        "    img[1] += 0.456\n",
        "    img[2] += 0.406\n",
        "\n",
        "    img = img.numpy().transpose((1, 2, 0))\n",
        "\n",
        "\n",
        "    plt.imshow(img)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-10T22:19:53.601538Z",
          "iopub.status.busy": "2024-12-10T22:19:53.601203Z",
          "iopub.status.idle": "2024-12-10T22:20:15.110998Z",
          "shell.execute_reply": "2024-12-10T22:20:15.110151Z",
          "shell.execute_reply.started": "2024-12-10T22:19:53.601498Z"
        },
        "id": "PdY3KmY4YrBR",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#Initiate the Dataset and Dataloader\n",
        "\n",
        "#setting the constants\n",
        "data_location =  \"../input/flickr30k\"\n",
        "BATCH_SIZE = 128\n",
        "NUM_WORKER = 4\n",
        "\n",
        "#defining the transform to be applied\n",
        "transforms = T.Compose([\n",
        "    T.Resize(226),  # Resize the image to\n",
        "    T.RandomCrop(224),  # Randomly crop a 224x224 patch\n",
        "    T.ToTensor(), # Chuyển đổi ảnh sang tensor\n",
        "    T.Normalize((0.485, 0.456, 0.406),(0.229, 0.224, 0.225)) # Chuẩn hóa giá trị pixel với giá trị mean và std\n",
        "])\n",
        "\n",
        "\n",
        "#thiết lập dataset TT bên trên nhưng khác phép biến đổi\n",
        "dataset =  FlickrDataset(\n",
        "    root_dir = data_location+\"/Images\",\n",
        "    captions_file = data_location+\"/captions.txt\",\n",
        "    transform=transforms\n",
        ")\n",
        "\n",
        "\n",
        "# tạo data_loader bằng hàm đã thiết lập bên trên\n",
        "data_loader = get_data_loader(\n",
        "    dataset=dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_workers=NUM_WORKER,\n",
        "    shuffle=True,\n",
        "    # batch_first = True\n",
        ")\n",
        "\n",
        "#vocab_size\n",
        "vocab_size = len(dataset.vocab)\n",
        "print(vocab_size)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POGmBRKAYrBR",
        "trusted": true
      },
      "source": [
        "### 3) Defining the Model Architecture\n",
        "\n",
        "Model is seq2seq model. In the **encoder** pretrained ResNet model is used to extract the features. Decoder, is the implementation of the Bahdanau Attention Decoder. In the decoder model **LSTM cell**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-10T22:20:15.112644Z",
          "iopub.status.busy": "2024-12-10T22:20:15.11229Z",
          "iopub.status.idle": "2024-12-10T22:20:15.117353Z",
          "shell.execute_reply": "2024-12-10T22:20:15.116489Z",
          "shell.execute_reply.started": "2024-12-10T22:20:15.112606Z"
        },
        "id": "gzel1mWWYrBR",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "import torchvision.transforms as T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-10T22:20:15.11859Z",
          "iopub.status.busy": "2024-12-10T22:20:15.118334Z",
          "iopub.status.idle": "2024-12-10T22:20:15.131131Z",
          "shell.execute_reply": "2024-12-10T22:20:15.13045Z",
          "shell.execute_reply.started": "2024-12-10T22:20:15.118566Z"
        },
        "id": "0chhtZKVYrBR",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "class EncoderCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EncoderCNN, self).__init__()\n",
        "        # Load Inception v3 with pretrained weights\n",
        "        inception = models.inception_v3(pretrained = True)\n",
        "\n",
        "        for param in inception.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        if hasattr(inception, 'AuxLogits'):\n",
        "            inception.AuxLogits = None\n",
        "\n",
        "        # Keep all layers except the fully connected and average pooling layers\n",
        "        self.inception = nn.Sequential(*list(inception.children())[:-3])  # Retain up to the convolutional layers\n",
        "\n",
        "    def forward(self, images):\n",
        "        # Forward pass through the Inception model\n",
        "        features = self.inception(images)  # (batch_size, 2048, 8, 8)\n",
        "        features = features.permute(0, 2, 3, 1)  # Change to (batch_size, 8, 8, 2048) for compatibility\n",
        "        features = features.view(features.size(0), -1, features.size(-1))\n",
        "        return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-10T22:20:15.132242Z",
          "iopub.status.busy": "2024-12-10T22:20:15.131961Z",
          "iopub.status.idle": "2024-12-10T22:20:15.150394Z",
          "shell.execute_reply": "2024-12-10T22:20:15.149586Z",
          "shell.execute_reply.started": "2024-12-10T22:20:15.132204Z"
        },
        "id": "xTUYbMY4YrBR",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "'''Bahdanau Attention: đầu tiên ta cần tính điểm score bằng công thức sau:\n",
        "Ei,t = A . tanh(U . fi + W . ht) với fi là output resnet, ht là hidden state thứ t của decode\n",
        "U,W là các ma trận trọng số cần học, A là vecto trọng số. Các tham số này cố định tại mọi t và i\n",
        "\n",
        "fi là vecto đặc trưng thứ i trong ma trận đặc trưng của RESNET. Ta biết output RESNET có dạng 49,2048 (49 row, 2048 column)\n",
        "==> có 49 vecto đặc trưng f với mỗi vecto có kích cỡ 2048\n",
        "\n",
        "ht là hidden state tại timestep t của lớp decode LSTM.\n",
        "\n",
        "Ei,t là 1 score đánh giá độ liên quan giữa state hiện tại của decode với vecto đặc trưng ảnh tương ứng.\n",
        "Ta cần cho nó đi qua hàm softmax để chuẩn hóa về khoảng [0,1], ta được tham số alpha tương ứng với từng vecto đặc trưng\n",
        "(có 49 alpha do có 49 vecto đặc trưng) ==> Alpha thay đổi theo từng state ht khác nhau\n",
        "\n",
        "Tại bước cuối, ta cần nhân alpha với vecto đặc trưng tương ứng rồi cộng tổng tất cả lại, ta được vecto đặc trưng ảnh\n",
        "ft với t tại timestep t và vecto này đã tập trung vào các phần model cần dự đoán trong state t+1 của decode\n",
        "\n",
        "'''\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, encoder_dim,decoder_dim,attention_dim):\n",
        "        super(Attention, self).__init__()\n",
        "\n",
        "        self.attention_dim = attention_dim\n",
        "\n",
        "\n",
        "        # tính attetion score\n",
        "        self.W = nn.Linear(decoder_dim,attention_dim)  # chuyển hidden state ra không gian attention\n",
        "        self.U = nn.Linear(encoder_dim,attention_dim)   # chuyển vecto đặc trưng ra không gian attention\n",
        "        self.A = nn.Linear(attention_dim,1) # ánh xạ không gian attention thành score\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, features, hidden_state):\n",
        "        u_hs = self.U(features)     #(batch_size,num_layers,attention_dim)\n",
        "        w_ah = self.W(hidden_state) #(batch_size,attention_dim)\n",
        "\n",
        "        combined_states = torch.tanh(u_hs + w_ah.unsqueeze(1)) #(batch_size,num_layers,attemtion_dim)\n",
        "\n",
        "        attention_scores = self.A(combined_states)         #(batch_size,num_layers,1)\n",
        "        attention_scores = attention_scores.squeeze(2)     #(batch_size,num_layers)\n",
        "\n",
        "\n",
        "        alpha = F.softmax(attention_scores,dim=1)          #(batch_size,num_layers)\n",
        "\n",
        "        attention_weights = features * alpha.unsqueeze(2)  #(batch_size,num_layers,features_dim)\n",
        "        attention_weights = attention_weights.sum(dim=1)   #(batch_size,num_layers)\n",
        "\n",
        "        return alpha,attention_weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-10T22:20:15.151749Z",
          "iopub.status.busy": "2024-12-10T22:20:15.15149Z",
          "iopub.status.idle": "2024-12-10T22:20:15.16962Z",
          "shell.execute_reply": "2024-12-10T22:20:15.168914Z",
          "shell.execute_reply.started": "2024-12-10T22:20:15.151726Z"
        },
        "id": "1h5J5PpGYrBS",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#Attention Decoder\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self,embed_size, vocab_size, attention_dim,encoder_dim,decoder_dim,drop_prob=0.3):\n",
        "        super().__init__()\n",
        "\n",
        "        #save the model param\n",
        "        self.vocab_size = vocab_size\n",
        "        self.attention_dim = attention_dim\n",
        "        self.decoder_dim = decoder_dim\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size,embed_size)\n",
        "        self.attention = Attention(encoder_dim,decoder_dim,attention_dim)\n",
        "\n",
        "\n",
        "        self.init_h = nn.Linear(encoder_dim, decoder_dim) # khởi tạo hidden state\n",
        "        self.init_c = nn.Linear(encoder_dim, decoder_dim) # khởi tạo cell state\n",
        "        self.lstm_cell = nn.LSTMCell(embed_size+encoder_dim,decoder_dim,bias=True)  # khởi tạo LSTM với 2 input lààà\n",
        "                                                                        # embedding vecto và vecto đặc trưng qua attetion\n",
        "\n",
        "\n",
        "\n",
        "        self.f_beta = nn.Linear(decoder_dim, encoder_dim) # tạo gating mechanism, giúp quản lý các gate trong LSTM\n",
        "\n",
        "\n",
        "        self.fcn = nn.Linear(decoder_dim,vocab_size) # tầng fcn để ánh xạ từ hidden state sang không gian vocab\n",
        "        self.drop = nn.Dropout(drop_prob)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, features, captions):\n",
        "\n",
        "        #vectorize the caption\n",
        "        embeds = self.embedding(captions)\n",
        "\n",
        "        # Initialize LSTM state\n",
        "        h, c = self.init_hidden_state(features)  # (batch_size, decoder_dim)\n",
        "\n",
        "        #get the seq length to iterate\n",
        "        seq_length = len(captions[0])-1     # độ dài của câu trừ token end\n",
        "        batch_size = captions.size(0) # số ảnh trong batch\n",
        "        num_features = features.size(1) # số vecto đặc trưng (49)\n",
        "\n",
        "        # khởi tạo tensor chứa kết quả dự đoán dưới dạng softmax tại mỗi timestep\n",
        "        preds = torch.zeros(batch_size, seq_length, self.vocab_size).to(device)\n",
        "        alphas = torch.zeros(batch_size, seq_length,num_features).to(device)\n",
        "        #  (batch_size, seq_length, num_features) là kích cỡ alphas dùng để lưu tất cả alpha trong attetion\n",
        "\n",
        "        for s in range(seq_length):\n",
        "            alpha,context = self.attention(features, h)\n",
        "            lstm_input = torch.cat((embeds[:, s], context), dim=1) # nối embedding với context vecto lại thành input LSTM\n",
        "            h, c = self.lstm_cell(lstm_input, (h, c))\n",
        "\n",
        "            output = self.fcn(self.drop(h)) # lớp fcn giúp chuyển hidden state mới nhất thành vecto output để đoán từ tiếp\n",
        "            # output có kích cỡ (batch_size, vocab_size) với mỗi dòng là kết quả softmax các từ trong bộ từ điển\n",
        "\n",
        "            # lưu lại các giá trị vừa có vào preds và alphas\n",
        "            preds[:,s] = output\n",
        "            alphas[:,s] = alpha\n",
        "\n",
        "\n",
        "        return preds, alphas # tensor các softmax tại mỗi timestep và tensor các alpha tại mỗi timestep\n",
        "\n",
        "\n",
        "    # using beam search\n",
        "    def generate_caption(self, features, max_len=20, vocab=None, beam_size=3):\n",
        "        # Inference part using beam search with log-probabilities\n",
        "        batch_size = features.size(0)\n",
        "        h, c = self.init_hidden_state(features)  # (batch_size, decoder_dim)\n",
        "\n",
        "        alphas = []\n",
        "\n",
        "        # Starting input (the <SOS> token)\n",
        "        word = torch.tensor(vocab.stoi['<SOS>']).view(1, -1).to(device)\n",
        "        embeds = self.embedding(word)\n",
        "\n",
        "        # Initialize beams with the first token and its probability\n",
        "        beams = [(embeds, h, c, [vocab.stoi['<SOS>']], 0)]  # (input embedding, h, c, list of previous word indices, score)\n",
        "\n",
        "        for step in range(max_len):  # At each timestep\n",
        "            all_candidates = []\n",
        "\n",
        "            # Expand each beam by generating the next word\n",
        "            for embeds, h, c, seq, score in beams:  # Iterate over each beam\n",
        "                # If this beam has finished (contains <EOS>), just add it to all_candidates without generating new word\n",
        "                if seq and seq[-1] == vocab.stoi['<EOS>']:\n",
        "                    all_candidates.append((embeds, h, c, seq, score))\n",
        "                    continue\n",
        "\n",
        "                # Calculate attention and context for each beam\n",
        "                alpha, context = self.attention(features, h)\n",
        "                alphas.append(alpha.cpu().detach().numpy())\n",
        "\n",
        "                lstm_input = torch.cat((embeds[:, 0], context), dim=1)\n",
        "                h, c = self.lstm_cell(lstm_input, (h, c))\n",
        "                output = self.fcn(self.drop(h))  # (batch_size, vocab_size)\n",
        "                output = output.view(batch_size, -1)\n",
        "\n",
        "                # Calculate log-probabilities of the next word\n",
        "                log_probs = torch.log_softmax(output, dim=1)  # Convert scores to log-probabilities\n",
        "\n",
        "                # Select the top beam_size words with the highest log-probabilities\n",
        "                topk_log_probs, topk_indices = torch.topk(log_probs, beam_size, dim=1)\n",
        "\n",
        "                for i in range(beam_size):\n",
        "                    # For each candidate, append the word and score\n",
        "                    word_idx = topk_indices[:, i]\n",
        "                    new_log_prob = topk_log_probs[:, i].item()  # Take the log probability of the word\n",
        "                    new_score = score + new_log_prob  # Add log-probability to the previous score\n",
        "                    new_seq = seq + [word_idx.item()]\n",
        "\n",
        "                    all_candidates.append((self.embedding(word_idx.unsqueeze(0)), h, c, new_seq, new_score))\n",
        "\n",
        "            # Sort all candidates by score (descending) and update beams\n",
        "            beams = sorted(all_candidates, key=lambda x: x[4], reverse=True)[:beam_size]\n",
        "\n",
        "        # The beam with the highest score will contain the best caption\n",
        "        best_beam = beams[0]\n",
        "        caption = best_beam[3]  # Get the sequence of words with the highest score\n",
        "\n",
        "        # Convert the sequence of word indices to words\n",
        "        caption_words = [vocab.itos[idx] for idx in caption]\n",
        "\n",
        "        return caption_words, alphas  # caption_words is the generated caption, alphas are the attention weights at each timestep\n",
        "\n",
        "\n",
        "\n",
        "    def init_hidden_state(self, encoder_out):\n",
        "        mean_encoder_out = encoder_out.mean(dim=1) # tính trung bình theo column ==> output kích cỡ là (batch_size,2048)\n",
        "        h = self.init_h(mean_encoder_out)  # (batch_size, decoder_dim)\n",
        "        c = self.init_c(mean_encoder_out)\n",
        "        return h, c\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emSkveqhYrBS",
        "trusted": true
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-10T22:20:15.171081Z",
          "iopub.status.busy": "2024-12-10T22:20:15.170724Z",
          "iopub.status.idle": "2024-12-10T22:20:15.185226Z",
          "shell.execute_reply": "2024-12-10T22:20:15.184467Z",
          "shell.execute_reply.started": "2024-12-10T22:20:15.171029Z"
        },
        "id": "FVSt0RudYrBS",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class EncoderDecoder(nn.Module):\n",
        "    def __init__(self,embed_size, vocab_size, attention_dim,encoder_dim,decoder_dim,drop_prob=0.3):\n",
        "        super().__init__()\n",
        "        self.encoder = EncoderCNN()\n",
        "        self.decoder = DecoderRNN(\n",
        "            embed_size=embed_size,\n",
        "            vocab_size = len(dataset.vocab),\n",
        "            attention_dim=attention_dim,\n",
        "            encoder_dim=encoder_dim,\n",
        "            decoder_dim=decoder_dim\n",
        "        )\n",
        "\n",
        "    def forward(self, images, captions):\n",
        "        features = self.encoder(images)\n",
        "        outputs = self.decoder(features, captions)\n",
        "        return outputs # (batch_size,seq_length,vocab_size).\n",
        "        # output có số row = độ dài câu và mỗi câu là giá trị softmax các từ phù hợp\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqX4OHgDYrBS"
      },
      "source": [
        "### 4) Setting Hypperparameter and Init the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-10T22:20:15.18639Z",
          "iopub.status.busy": "2024-12-10T22:20:15.186155Z",
          "iopub.status.idle": "2024-12-10T22:20:15.198984Z",
          "shell.execute_reply": "2024-12-10T22:20:15.198356Z",
          "shell.execute_reply.started": "2024-12-10T22:20:15.186367Z"
        },
        "id": "aczWIshwYrBS",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#Hyperparams\n",
        "embed_size=300\n",
        "vocab_size = len(dataset.vocab)\n",
        "attention_dim=256\n",
        "encoder_dim=2048\n",
        "decoder_dim=512\n",
        "learning_rate = 3e-4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-10T22:20:15.200046Z",
          "iopub.status.busy": "2024-12-10T22:20:15.199799Z",
          "iopub.status.idle": "2024-12-10T22:20:18.899136Z",
          "shell.execute_reply": "2024-12-10T22:20:18.898191Z",
          "shell.execute_reply.started": "2024-12-10T22:20:15.200023Z"
        },
        "id": "FGMZpbaiYrBS",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#init model\n",
        "model = EncoderDecoder(\n",
        "    embed_size=300,\n",
        "    vocab_size = len(dataset.vocab),\n",
        "    attention_dim=256,\n",
        "    encoder_dim=2048,\n",
        "    decoder_dim=512\n",
        ").to(device)\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"])\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-10T22:20:18.900443Z",
          "iopub.status.busy": "2024-12-10T22:20:18.900175Z",
          "iopub.status.idle": "2024-12-10T22:20:18.905263Z",
          "shell.execute_reply": "2024-12-10T22:20:18.904377Z",
          "shell.execute_reply.started": "2024-12-10T22:20:18.900418Z"
        },
        "id": "P8kMqGsVYrBS",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#helper function to save the model\n",
        "def save_model(model, num_epochs):\n",
        "    model_state = {\n",
        "        'num_epochs': num_epochs,              # Số epoch đã huấn luyện\n",
        "        'embed_size': embed_size,              # Kích thước embedding\n",
        "        'vocab_size': len(dataset.vocab),      # Kích thước từ điển\n",
        "        'attention_dim': attention_dim,        # Kích thước attention\n",
        "        'encoder_dim': encoder_dim,            # Kích thước đầu ra encoder\n",
        "        'decoder_dim': decoder_dim,            # Kích thước hidden state trong decoder\n",
        "        'state_dict': model.state_dict()       # Trạng thái các tham số của mô hình\n",
        "    }\n",
        "\n",
        "    torch.save(model_state, 'attention_LSTM_model_state.pth')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SA4c8tvoYrBS"
      },
      "source": [
        "## 5) Training Job from above configs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-10T22:20:18.906951Z",
          "iopub.status.busy": "2024-12-10T22:20:18.906651Z"
        },
        "id": "rD6roj5KYrBS",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "num_epochs = 40\n",
        "print_every = 200\n",
        "\n",
        "for epoch in range(1,num_epochs+1):\n",
        "    for idx, (image, captions) in enumerate(iter(data_loader)):\n",
        "        image,captions = image.to(device),captions.to(device)\n",
        "\n",
        "        # Zero the gradients.\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Feed forward\n",
        "        outputs,attentions = model(image, captions)\n",
        "\n",
        "        # Calculate the batch loss.\n",
        "        targets = captions[:,1:]\n",
        "        loss = criterion(outputs.view(-1, vocab_size), targets.reshape(-1))\n",
        "\n",
        "        # Backward pass.\n",
        "        loss.backward()\n",
        "\n",
        "        # Update the parameters in the optimizer.\n",
        "        optimizer.step()\n",
        "\n",
        "        if (idx+1)%print_every == 0:\n",
        "            print(\"Epoch: {} loss: {:.5f}\".format(epoch,loss.item()))\n",
        "\n",
        "\n",
        "            #generate the caption\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                dataiter = iter(data_loader)\n",
        "                img,_ = next(dataiter)\n",
        "                features = model.encoder(img[0:1].to(device))\n",
        "                caps,alphas = model.decoder.generate_caption(features,vocab=dataset.vocab)\n",
        "                caption = ' '.join(caps)\n",
        "                show_image(img[0],title=caption)\n",
        "\n",
        "            model.train()\n",
        "\n",
        "    #save the latest model\n",
        "    save_model(model,epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pa-QU-PMYrBS"
      },
      "source": [
        "## 6 Visualizing the attentions\n",
        "Defining helper functions\n",
        "<li>Given the image generate captions and attention scores</li>\n",
        "<li>Plot the attention scores in the image</li>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-cpsgZjHYrBS",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#generate caption\n",
        "def get_caps_from(features_tensors):\n",
        "    #generate the caption\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        features = model.encoder(features_tensors.to(device))\n",
        "        caps,alphas = model.decoder.generate_caption(features,vocab=dataset.vocab)\n",
        "        caption = ' '.join(caps)\n",
        "        show_image(features_tensors[0],title=caption)\n",
        "\n",
        "    return caps,alphas\n",
        "\n",
        "#Show attention\n",
        "def plot_attention(img, result, attention_plot):\n",
        "    #untransform\n",
        "    img[0] = img[0] * 0.229\n",
        "    img[1] = img[1] * 0.224\n",
        "    img[2] = img[2] * 0.225\n",
        "    img[0] += 0.485\n",
        "    img[1] += 0.456\n",
        "    img[2] += 0.406\n",
        "\n",
        "    img = img.numpy().transpose((1, 2, 0))\n",
        "    temp_image = img\n",
        "\n",
        "    fig = plt.figure(figsize=(15, 15))\n",
        "\n",
        "    len_result = len(result)\n",
        "    for l in range(len_result):\n",
        "        temp_att = attention_plot[l].reshape(7,7)\n",
        "\n",
        "        ax = fig.add_subplot(len_result//2,len_result//2, l+1)\n",
        "        ax.set_title(result[l])\n",
        "        img = ax.imshow(temp_image)\n",
        "        ax.imshow(temp_att, cmap='gray', alpha=0.7, extent=img.get_extent())\n",
        "\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9gbck9qYrBT",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#show any 1\n",
        "dataiter = iter(data_loader)\n",
        "images,_ = next(dataiter)\n",
        "\n",
        "img = images[0].detach().clone()\n",
        "img1 = images[0].detach().clone()\n",
        "caps,alphas = get_caps_from(img.unsqueeze(0))\n",
        "\n",
        "plot_attention(img1, caps, alphas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rw5YNfCYrBT",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#show any 1\n",
        "dataiter = iter(data_loader)\n",
        "images,_ = next(dataiter)\n",
        "\n",
        "img = images[0].detach().clone()\n",
        "img1 = images[0].detach().clone()\n",
        "caps,alphas = get_caps_from(img.unsqueeze(0))\n",
        "\n",
        "plot_attention(img1, caps, alphas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KnmmKbd9YrBT",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#show any 1\n",
        "dataiter = iter(data_loader)\n",
        "images,_ = next(dataiter)\n",
        "\n",
        "img = images[0].detach().clone()\n",
        "img1 = images[0].detach().clone()\n",
        "caps,alphas = get_caps_from(img.unsqueeze(0))\n",
        "\n",
        "plot_attention(img1, caps, alphas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1bhdEYLvYrBW",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#show any 1\n",
        "dataiter = iter(data_loader)\n",
        "images,_ = next(dataiter)\n",
        "\n",
        "img = images[0].detach().clone()\n",
        "img1 = images[0].detach().clone()\n",
        "caps,alphas = get_caps_from(img.unsqueeze(0))\n",
        "\n",
        "plot_attention(img1, caps, alphas)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 623289,
          "sourceId": 1111676,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 623329,
          "sourceId": 1111749,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 868251,
          "sourceId": 1479515,
          "sourceType": "datasetVersion"
        }
      ],
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
